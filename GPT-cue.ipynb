{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1418659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GEMUnrolledEM(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, num_layers=5, eta=0.1, use_detach=False):\n",
    "        \"\"\"\n",
    "        GEM Unrolled EM Network\n",
    "        Args:\n",
    "            num_nodes: 图节点数\n",
    "            num_layers: 展开层数 K\n",
    "            eta: M-step 学习率\n",
    "            use_detach: 是否在 M-step 中 detach gradient\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_layers = num_layers\n",
    "        self.eta = eta\n",
    "        self.use_detach = use_detach\n",
    "        \n",
    "        # theta 是生成 W 的参数\n",
    "        self.theta = nn.Parameter(torch.rand(num_nodes, num_nodes))\n",
    "        \n",
    "    def generate_W(self, theta):\n",
    "        \"\"\"用 theta 生成图权重 W, 可以是非线性映射\"\"\"\n",
    "        # 例如简单映射 + softplus 保证非负\n",
    "        W = torch.nn.functional.softplus(theta)\n",
    "        # 对角置零，避免自环\n",
    "        W = W - torch.diag(torch.diag(W))\n",
    "        return W\n",
    "\n",
    "    def compute_laplacian(self, W):\n",
    "        \"\"\"计算图 Laplacian\"\"\"\n",
    "        D = torch.diag(W.sum(dim=1))\n",
    "        L = D - W\n",
    "        return L\n",
    "\n",
    "    def E_step(self, L, x, eps=1e-3):\n",
    "        \"\"\"E-step: 计算潜在信号 z_hat\"\"\"\n",
    "        N = L.shape[0]\n",
    "        L_reg = L + eps * torch.eye(N, device=L.device)\n",
    "        cov = torch.linalg.inv(L_reg)\n",
    "        cov = (cov + cov.T) / 2\n",
    "        z_hat = cov @ x  # 简化版后验均值\n",
    "        return z_hat\n",
    "\n",
    "    def M_step(self, theta, z_hat):\n",
    "        \"\"\"\n",
    "        M-step: 用梯度更新 theta\n",
    "        Q 函数依赖 theta 生成的 W\n",
    "        \"\"\"\n",
    "        W = self.generate_W(theta)\n",
    "        # 构造示例损失 Q (这里假设 Q = ||W - z_hat||^2)\n",
    "        Q = ((W - z_hat) ** 2).sum()\n",
    "\n",
    "        # 对 theta 求梯度\n",
    "        grad = torch.autograd.grad(Q, theta, create_graph=not self.use_detach)[0]\n",
    "\n",
    "        if self.use_detach:\n",
    "            grad = grad.detach()\n",
    "\n",
    "        # 更新 theta\n",
    "        theta_next = theta - self.eta * grad\n",
    "        return theta_next\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [num_nodes, feature_dim] 输入\n",
    "        \"\"\"\n",
    "        theta = self.theta\n",
    "        for k in range(self.num_layers):\n",
    "            # 1. 生成 W\n",
    "            W = self.generate_W(theta)\n",
    "\n",
    "            # 2. 计算 Laplacian\n",
    "            L = self.compute_laplacian(W)\n",
    "\n",
    "            # 3. E-step\n",
    "            z_hat = self.E_step(L, x)\n",
    "\n",
    "            # 4. M-step\n",
    "            theta = self.M_step(theta, z_hat)\n",
    "\n",
    "        # 返回最终生成的 W 和后验均值 z_hat\n",
    "        W_final = self.generate_W(theta)\n",
    "        return W_final, z_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f7ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 6\n",
    "feature_dim = 1\n",
    "x = torch.rand(num_nodes, feature_dim)\n",
    "\n",
    "model = GEMUnrolledEM(num_nodes=num_nodes, num_layers=3, eta=0.1, use_detach=False)\n",
    "theta_final, z_hat_final = model(x)\n",
    "\n",
    "# 定义最终任务损失，比如目标信号 y\n",
    "y = torch.rand(num_nodes, feature_dim)\n",
    "loss = ((z_hat_final - y)**2).sum()\n",
    "\n",
    "# 端到端训练\n",
    "loss.backward()  # 自动反向传播到 W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67913488",
   "metadata": {},
   "source": [
    "假设 M-step 是通过最优化得到参数 θ*：\n",
    "$$\n",
    "\\theta^* = \\arg\\max_\\theta Q(\\theta, z)\n",
    "$$\n",
    "M-step 的最优条件（零梯度条件）：\n",
    "$$\n",
    "\\nabla_\\theta Q(\\theta^*, z) = 0\n",
    "$$\n",
    "利用 隐函数求导，可以得到 θ* 对 z 的梯度：\n",
    "$$\n",
    "\\frac{d \\theta^*}{dz} = - (\\nabla^2_{\\theta\\theta} Q(\\theta^*, z))^{-1} \\nabla^2_{\\theta z} Q(\\theta^*, z)\n",
    "$$\n",
    "\t•\tforward：求 θ*（M-step 输出）\n",
    "\t•\tbackward：用 torch.autograd.functional.hvp 或 Neumann series 近似 Hessian inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class GEMImplicitEM(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, num_layers=5, eta=0.1):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_layers = num_layers\n",
    "        self.eta = eta\n",
    "        self.W = torch.nn.Parameter(torch.rand(num_nodes, num_nodes))\n",
    "    \n",
    "    def compute_laplacian(self, W):\n",
    "        D = torch.diag(W.sum(dim=1))\n",
    "        L = D - W\n",
    "        return L\n",
    "    \n",
    "    def E_step(self, L, x, eps=1e-3):\n",
    "        N = L.shape[0]\n",
    "        L_reg = L + eps * torch.eye(N, device=L.device)\n",
    "        cov = torch.linalg.inv(L_reg)\n",
    "        cov = (cov + cov.T)/2\n",
    "        z_hat = cov @ x\n",
    "        return z_hat\n",
    "    \n",
    "    def M_step(self, theta, z_hat, n_iter=10):\n",
    "        \"\"\"\n",
    "        M-step using implicit differentiation.\n",
    "        theta: [N, N]\n",
    "        z_hat: [N, F]\n",
    "        \"\"\"\n",
    "        # forward: gradient update iteration (K steps)\n",
    "        theta_k = theta\n",
    "        for _ in range(n_iter):\n",
    "            Q = ((theta_k - z_hat)**2).sum()\n",
    "            grad = torch.autograd.grad(Q, theta_k, create_graph=True)[0]\n",
    "            theta_k = theta_k - self.eta * grad  # forward pass\n",
    "\n",
    "        theta_star = theta_k.detach()  # detach to avoid storing all intermediate grads\n",
    "\n",
    "        # backward: implicit differentiation\n",
    "        theta_star.requires_grad_(True)\n",
    "        Q_star = ((theta_star - z_hat)**2).sum()\n",
    "\n",
    "        # Hessian-vector product function\n",
    "        def Hv(v):\n",
    "            return torch.autograd.grad(\n",
    "                torch.autograd.grad(Q_star, theta_star, create_graph=True)[0] @ v,\n",
    "                theta_star,\n",
    "                retain_graph=True\n",
    "            )[0]\n",
    "\n",
    "        # Solve (I - eta * H)^{-1} * grad_output approximately using Neumann series\n",
    "        def neumann_solve(grad_out, K=5):\n",
    "            v = grad_out\n",
    "            approx = v.clone()\n",
    "            for _ in range(K):\n",
    "                Hv_v = Hv(v)\n",
    "                v = eta * Hv_v\n",
    "                approx = approx + v\n",
    "            return approx\n",
    "\n",
    "        return theta_star, neumann_solve\n",
    "\n",
    "    def forward(self, x):\n",
    "        theta = self.W\n",
    "        for k in range(self.num_layers):\n",
    "            L = self.compute_laplacian(theta)\n",
    "            z_hat = self.E_step(L, x)\n",
    "            theta, neumann_solver = self.M_step(theta, z_hat)\n",
    "        return theta, z_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0497241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LaplacianPseudoInverse:\n",
    "    def __init__(self, method='neumann', eps=1e-3, K=5, rank=None, maxiter=50):\n",
    "        \"\"\"\n",
    "        Laplacian 广义逆计算器\n",
    "        Args:\n",
    "            method: 'neumann' | 'cg' | 'lowrank'\n",
    "            eps: 正则化参数，避免奇异\n",
    "            K: Neumann series 或迭代次数\n",
    "            rank: low-rank 截断特征数量\n",
    "            maxiter: CG 最大迭代次数\n",
    "        \"\"\"\n",
    "        assert method in ['neumann', 'cg', 'lowrank'], \"method must be one of 'neumann','cg','lowrank'\"\n",
    "        self.method = method\n",
    "        self.eps = eps\n",
    "        self.K = K\n",
    "        self.rank = rank\n",
    "        self.maxiter = maxiter\n",
    "\n",
    "    def compute(self, L):\n",
    "        if self.method == 'neumann':\n",
    "            return self._neumann(L)\n",
    "        elif self.method == 'cg':\n",
    "            return self._cg(L)\n",
    "        elif self.method == 'lowrank':\n",
    "            return self._lowrank(L)\n",
    "    \n",
    "    # ----------------------\n",
    "    # Neumann series 近似\n",
    "    # ----------------------\n",
    "    def _neumann(self, L_sparse):\n",
    "        N = L_sparse.shape[0]\n",
    "        device = L_sparse.device\n",
    "        I_sparse = torch.sparse_coo_tensor(\n",
    "            indices=torch.stack([torch.arange(N), torch.arange(N)]),\n",
    "            values=torch.ones(N, device=device),\n",
    "            size=(N,N)\n",
    "        )\n",
    "        M = I_sparse - L_sparse / self.eps\n",
    "        M_power = I_sparse.clone()\n",
    "        L_pinv_approx = M_power.clone()\n",
    "        for _ in range(self.K):\n",
    "            M_power = torch.sparse.mm(M_power, M)\n",
    "            L_pinv_approx = torch.sparse.add(L_pinv_approx, M_power)\n",
    "        L_pinv_approx = L_pinv_approx * (1.0 / self.eps)\n",
    "        return L_pinv_approx\n",
    "\n",
    "    # ----------------------\n",
    "    # Conjugate Gradient 迭代求逆列\n",
    "    # ----------------------\n",
    "    def _cg(self, L):\n",
    "        N = L.shape[0]\n",
    "        device = L.device\n",
    "        I = torch.eye(N, device=device)\n",
    "        L_reg = L + self.eps * I\n",
    "        L_pinv_approx = torch.zeros_like(L)\n",
    "        for i in range(N):\n",
    "            e = I[:, i]\n",
    "            x, _ = torch.linalg.cg(L_reg, e, maxiter=self.maxiter)\n",
    "            L_pinv_approx[:, i] = x\n",
    "        return L_pinv_approx\n",
    "\n",
    "    # ----------------------\n",
    "    # 低秩特征截断近似\n",
    "    # ----------------------\n",
    "    def _lowrank(self, L):\n",
    "        N = L.shape[0]\n",
    "        device = L.device\n",
    "        eigvals, eigvecs = torch.linalg.eigh(L)\n",
    "        if self.rank is None:\n",
    "            self.rank = N - 1  # 默认去掉零特征\n",
    "        # 取最后 rank 个非零特征\n",
    "        Lambda_inv = torch.diag(1.0 / eigvals[-self.rank:])\n",
    "        L_pinv_approx = eigvecs[:, -self.rank:] @ Lambda_inv @ eigvecs[:, -self.rank:].T\n",
    "        return L_pinv_approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c8ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 L 是稀疏 Laplacian\n",
    "num_nodes = 6\n",
    "edges = torch.tensor([[0,1],[0,2],[1,2],[2,3],[3,4],[4,5],[3,5]])\n",
    "weights = torch.tensor([0.6]*len(edges))\n",
    "adj = torch.zeros(num_nodes, num_nodes)\n",
    "for (i,j), w in zip(edges.tolist(), weights):\n",
    "    adj[i,j] = w\n",
    "    adj[j,i] = w\n",
    "D = torch.diag(adj.sum(dim=1))\n",
    "L = D - adj\n",
    "\n",
    "# ----------------------\n",
    "# 方法1: Neumann series\n",
    "# ----------------------\n",
    "L_sparse = L.to_sparse()\n",
    "lp_inv_neumann = LaplacianPseudoInverse(method='neumann', eps=1e-3, K=5)\n",
    "L_pinv = lp_inv_neumann.compute(L_sparse)\n",
    "print(L_pinv)\n",
    "\n",
    "# ----------------------\n",
    "# 方法2: CG 迭代\n",
    "# ----------------------\n",
    "lp_inv_cg = LaplacianPseudoInverse(method='cg', eps=1e-3, maxiter=50)\n",
    "L_pinv_cg = lp_inv_cg.compute(L)\n",
    "print(L_pinv_cg)\n",
    "\n",
    "# ----------------------\n",
    "# 方法3: 低秩截断\n",
    "# ----------------------\n",
    "lp_inv_lowrank = LaplacianPseudoInverse(method='lowrank', rank=5)\n",
    "L_pinv_lowrank = lp_inv_lowrank.compute(L)\n",
    "print(L_pinv_lowrank)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
