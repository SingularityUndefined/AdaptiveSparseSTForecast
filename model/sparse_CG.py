import torch

'''
This document implements the  Conjugate Gradient (CG) methods:
- Conjugate Gradient (CG)
- block CG
- batch CG
- Preconditioned Conjugate Gradient (PCG)
'''

def conjugated_gradients(A_func, b, x0=None, tol=1e-6, max_iter=100):
    # TODO: generated by GPT, need to verify correctness
    
    """
    Solve Ax = b using Conjugate Gradient method where A is given as a function.
    A_func: function that takes a tensor x and returns Ax
    b: right-hand side tensor
    x0: initial guess (optional)
    tol: tolerance for convergence
    max_iter: maximum number of iterations
    """
    if x0 is None:
        x = torch.zeros_like(b)
    else:
        x = x0.clone()

    r = b - A_func(x)
    p = r.clone()
    rsold = torch.dot(r.view(-1), r.view(-1))

    for i in range(max_iter):
        Ap = A_func(p)
        alpha = rsold / (torch.dot(p.view(-1), Ap.view(-1)) + 1e-8)
        x += alpha * p
        r -= alpha * Ap
        rsnew = torch.dot(r.view(-1), r.view(-1))
        if torch.sqrt(rsnew) < tol:
            break
        p = r + (rsnew / rsold) * p
        rsold = rsnew

    return x


def batch_CG(A_func, B_vecs, X0=None, tol=1e-6, max_iter=None):
    """
    Solve A x = b for multiple RHS (batch) using independent CG in batch.
    Inputs are shaped (B, N), outputs also (B, N).

    Args:
        A_func: callable, A_func(X) returns A @ X, X shape (B, N)
        B_vecs: (B, N) tensor of RHS
        X0: initial guess (B, N)
        tol: convergence tolerance
        max_iter: maximum iterations

    Returns:
        X: solution (B, N)
        res_norms: list of residual norms per iteration (B,)
    """
    B_batch, N = B_vecs.shape
    if X0 is None:
        X = torch.zeros_like(B_vecs)
    else:
        X = X0.clone()

    R = B_vecs - A_func(X)   # (B, N)
    P = R.clone()
    rs_old = (R * R).sum(dim=1)  # shape (B,)

    if max_iter is None:
        max_iter = N

    res_norms = []
    for k in range(max_iter):
        AP = A_func(P)  # (B, N)
        alpha = rs_old / ((P * AP).sum(dim=1) + 1e-12)  # shape (B,)
        X = X + P * alpha.unsqueeze(1)
        R = R - AP * alpha.unsqueeze(1)
        rs_new = (R * R).sum(dim=1)
        res_norms.append(rs_new.sqrt())

        # check convergence for all batches
        if torch.all(rs_new.sqrt() < tol):
            break

        beta = rs_new / (rs_old + 1e-12)
        P = R + P * beta.unsqueeze(1)
        rs_old = rs_new

    return X, res_norms

import torch

def block_cg_func(A_func, B_vecs, X0=None, tol=1e-6, max_iter=None):
    """
    Solve AX = B using Block CG for multiple RHS in batch format (B, N).

    Args:
        A_func: callable, A_func(X) returns A @ X, shape (B, N)
        B_vecs: (B, N) tensor of RHS
        X0: initial guess (B, N)
        tol: convergence tolerance
        max_iter: maximum iterations

    Returns:
        X: solution (B, N)
        res_norms: list of residual norms per iteration (B,)
    """
    B_batch, N = B_vecs.shape
    if X0 is None:
        X = torch.zeros_like(B_vecs)
    else:
        X = X0.clone()

    R = B_vecs - A_func(X)  # (B, N)
    P = R.clone()
    # Block CG 中步长是矩阵 alpha: (B, B)
    # 这里先用对角线近似，可选
    rs_old = (R @ R.T)  # (B, B)

    if max_iter is None:
        max_iter = N

    res_norms = []
    for k in range(max_iter):
        AP = A_func(P)  # (B, N)
        # 计算矩阵 α: α = (P^T AP)^{-1} (R^T R)
        PtAP = P @ AP.T  # (B, B)
        RtR = R @ R.T    # (B, B)
        # 为了稳定性，可以用对角线近似
        alpha = torch.linalg.solve(PtAP, RtR)  # (B, B)
        X = X + alpha @ P        # (B, N)
        R = R - alpha @ AP       # (B, N)

        # 残差范数
        rs_new = torch.sum(R*R, dim=1)
        res_norms.append(rs_new.sqrt())

        # 收敛判断
        if torch.all(rs_new.sqrt() < tol):
            break

        # 更新 β
        beta = torch.linalg.solve(RtR, R @ R.T)  # 也可对角线近似
        P = R + beta @ P
        rs_old = R @ R.T

    return X, res_norms


def block_CG(A_func, b, x0=None, tol=1e-6, max_iter=100):
    """
    Solve AX = B using Block Conjugate Gradient method where A is given as a function.
    A_func: function that takes a tensor X and returns AX
    b: right-hand side tensor (B, N)
    x0: initial guess (optional)
    tol: tolerance for convergence
    max_iter: maximum number of iterations
    """
    if x0 is None:
        x = torch.zeros_like(b)
    else:
        x = x0.clone()

    r = b - A_func(x)
    p = r.clone()
    rtr = r.T @ r
    # rsold = torch.trace(r.t() @ r)

    for _ in range(max_iter):
        Ap = A_func(p)
        # pAp = torch.trace(p.t() @ Ap)
        # alpha = rsold / (pAp + 1e-8)
        ptAp = p.T @ Ap
        alpha = torch.linalg.solve(ptAp, rtr) # (B, B)
        x = x + p @ alpha
        r_new = r - Ap @ alpha
        # r = r - alpha * Ap
        # rsnew = torch.trace(r.t() @ r)
        if torch.norm(r) < tol:
            break
        rtr_new = r_new.T @ r_new
        beta = torch.linalg.solve(rtr, rtr_new) # rsnew / rsold
        p = r_new + p @ beta
        r = r_new
        rtr = rtr_new

    return x


def batch_PCG(A_func, B_vecs, x0=None, Minv_func=None, max_iter=50, tol=1e-6):
    """
    Batch Preconditioned Conjugate Gradient (PCG)
    Supports B systems in parallel, shape B x N
    
    Parameters
    ----------
    A_func : function
        Computes A @ X, X shape (B, N), returns (B, N)
    B_vecs : torch.Tensor, shape (B, N)
        Right-hand side vectors
    x0 : torch.Tensor, optional, shape (B, N)
    Minv_func : function, optional
        Computes M^{-1} @ X, shape (B, N)
    max_iter : int
    tol : float

    Returns
    -------
    X : torch.Tensor, shape (B, N)
        Solution vectors
    residuals : list of torch.Tensor
        Each element is (B, N) residual matrix at that iteration
    """
    B, N = B_vecs.shape
    if x0 is None:
        X = torch.zeros_like(B_vecs)
    else:
        X = x0.clone()
    
    R = B_vecs - A_func(X)   # initial residuals
    residuals = [R.clone().norm(dim=1)]  # store residual norms
    
    if Minv_func is not None:
        Z = Minv_func(R)      # shape (B, N)
    else:
        Z = R.clone()
    
    P = Z.clone()
    RZ_old = torch.sum(R * Z, dim=1)  # shape (B,)
    
    for k in range(max_iter):
        AP = A_func(P)  # shape (B, N)
        alpha = RZ_old / torch.sum(P * AP, dim=1)  # shape (B,)
        alpha = alpha.view(B, 1)
        
        X = X + alpha * P
        R = R - alpha * AP
        residuals.append(R.clone().norm(dim=1))
        
        # check convergence
        if torch.max(torch.norm(R, dim=1) / torch.norm(B_vecs, dim=1)) < tol:
            break
        
        if Minv_func is not None:
            Z = Minv_func(R)
        else:
            Z = R.clone()
        
        RZ_new = torch.sum(R * Z, dim=1)
        beta = (RZ_new / RZ_old).view(B, 1)
        P = Z + beta * P
        RZ_old = RZ_new
    
    return X, residuals